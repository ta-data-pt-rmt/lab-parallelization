{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelization Lab\n",
    "\n",
    "In this lab, you will be leveraging several concepts you have learned to obtain a list of links from a web page and crawl and index the pages referenced by those links - both sequentially and in parallel. Follow the steps below to complete the lab.\n",
    "\n",
    "### Step 1: Use the requests library to retrieve the content from the URL below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/Data_science'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "\n",
    "response.content[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Use BeautifulSoup to extract a list of all the unique links on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content)\n",
    "\n",
    "links = soup.find_all('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Use list comprehensions with conditions to clean the link list.\n",
    "\n",
    "There are two types of links, absolute and relative. Absolute links have the full URL and begin with *http* while relative links begin with a forward slash (/) and point to an internal page within the *wikipedia.org* domain. Clean the respective types of URLs as follows.\n",
    "\n",
    "- Absolute Links: Create a list of these and remove any that contain a percentage sign (%).\n",
    "- Relative Links: Create a list of these, add the domain to the link so that you have the full URL, and remove any that contain a percentage sign (%).\n",
    "- Combine the list of absolute and relative links and ensure there are no duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'http://wikipedia.org'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "absolute_links = [link for link in links if 'href=\"http' in str(link)]\n",
    "absolute_links = [re.findall('href=\"([^\"]+)', str(link)) for link in absolute_links]\n",
    "absolute_links = [link[0].replace('http', ' http') for link in absolute_links if link]\n",
    "absolute_links = [link.split(' ') for link in absolute_links]\n",
    "absolute_links = [link for sublist in absolute_links for link in sublist if link]\n",
    "absolute_links = [link for link in absolute_links if '%' not in link]\n",
    "\n",
    "relative_links = [link for link in links if 'href=\"/' in str(link)]\n",
    "relative_links = [re.findall('/wiki/[^\"]+', str(link)) for link in relative_links]\n",
    "relative_links = [domain + link[0] for link in relative_links if link]\n",
    "relative_links = [link for link in relative_links if '%' not in link]\n",
    "\n",
    "links = absolute_links + relative_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Use the os library to create a folder called *wikipedia* and make that the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('wikipedia')\n",
    "os.chdir('wikipedia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Write a function called index_page that accepts a link and does the following.\n",
    "\n",
    "- Tries to request the content of the page referenced by that link.\n",
    "- Slugifies the filename using the `slugify` function from the [python-slugify](https://pypi.org/project/python-slugify/) library and adds a .html file extension.\n",
    "    - If you don't already have the python-slugify library installed, you can pip install it as follows: `$ pip3 install python-slugify`.\n",
    "    - To import the slugify function, you would do the following: `from slugify import slugify`.\n",
    "    - You can then slugify a link as follows `slugify(link)`.\n",
    "- Creates a file in the wikipedia folder using the slugified filename and writes the contents of the page to the file.\n",
    "- If an exception occurs during the process above, just `pass`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from slugify import slugify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_page(link):\n",
    "   try:\n",
    "      response = requests.get(link)\n",
    "      soup = BeautifulSoup(response.content)\n",
    "      file_name = slugify(link) + '.html'\n",
    "      with open(file_name, 'w') as file:\n",
    "         file.write(str(soup))\n",
    "   except:\n",
    "      print(link, 'failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Sequentially loop through the list of links, running the index_page function each time.\n",
    "\n",
    "Remember to include `%%time` at the beginning of the cell so that it measures the time it takes for the cell to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "\n",
    "for link in links[:10]:\n",
    "   index_page(link)\n",
    "   time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Perform the page indexing in parallel and note the difference in performance.\n",
    "\n",
    "Remember to include `%%time` at the beginning of the cell so that it measures the time it takes for the cell to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "# Multiprocessing in Python on Windows using VS-Code + Jupyter/Ipython is bugged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "processes = []\n",
    "\n",
    "for link in links[:20]:\n",
    "   p = threading.Thread(target = index_page, args = [link])\n",
    "   p.start()\n",
    "   processes.append(p)\n",
    "\n",
    "for process in processes:\n",
    "   process.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
